# CORRECTED VERSION - batch_processing.py

import logging
import torch
import torch_neuronx
from benchmark import benchmark
from bert_encode import BERTEncoder

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(message)s',
    datefmt='%H:%M:%S'
)

def main():
    logging.info("üöÄ Starting BERT batch processing...")
    
    # Initialize encoder ONCE (more efficient)
    logging.info("üì• Initializing BERTEncoder...")
    encoder = BERTEncoder(model_name="bert-base-cased-finetuned-mrpc")  # Specify model name
    logging.info("‚úÖ Encoder initialized")
    
    # Compile BERT for different batch sizes
    logging.info("üîß Starting model compilation for different batch sizes...")
    
    try:
        for batch_size in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:
            logging.info(f"‚ö° Compiling model for batch size {batch_size}...")
            
            # Use your encoder class properly
            example = encoder.encode(encoder.sequence_0, encoder.sequence_2, batch_size=batch_size)
            
            # Trace using the encoder's model
            model_neuron = torch_neuronx.trace(encoder.model, example)
            
            # Save the traced model
            filename = f'model_batch_size_{batch_size}.pt'
            torch.jit.save(model_neuron, filename)
            logging.info(f"‚úÖ Saved {filename}")
            
    except Exception as e:
        logging.error(f"‚ùå Compilation error: {e}")
        import traceback
        traceback.print_exc()
        exit(1)
    
    logging.info("üéØ Model compilation completed!")
    
    # Benchmark BERT for different batch sizes
    logging.info("üìä Starting benchmarking...")
    
    try:
        for batch_size in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:
            print('-'*60)
            logging.info(f"üèÉ Benchmarking batch size {batch_size}...")
            
            # Create example for this batch size
            example = encoder.encode(encoder.sequence_0, encoder.sequence_2, batch_size=batch_size)
            filename = f'model_batch_size_{batch_size}.pt'
            
            # Run benchmark
            metrics = benchmark(filename, example)
            
            # Log key metrics
            logging.info(f"‚úÖ Batch {batch_size}: {metrics['throughput']:.2f} inferences/sec")
            print()
            
    except Exception as e:
        logging.error(f"‚ùå Benchmark execution failed: {e}")
        import traceback
        traceback.print_exc()
        exit(1)
    
    logging.info("üéâ All benchmarking completed successfully!")

if __name__ == "__main__":
    main()